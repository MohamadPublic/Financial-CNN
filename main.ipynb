{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a392fa-4521-48ee-b69f-89c521a5ecd8",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ecc7ed-9065-4f25-a339-bf2f9901051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import GPUtil\n",
    "import pandas as pd\n",
    "import yfinance as y\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from datetime import timedelta\n",
    "from scipy.spatial.distance import euclidean\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec7b36-b603-4f84-b634-ca72b838e361",
   "metadata": {},
   "source": [
    "# Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a821e63-83f9-49bb-b7ad-f1ecd88ec555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for reproducibility\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(314)\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(1618)\n",
    "\n",
    "# Set TensorFlow threading\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a2498-1796-4b82-9330-625ebbfa5dd9",
   "metadata": {},
   "source": [
    "# Establish Link to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be9e55-0a7b-4397-8bca-0554bd1d6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "\n",
    "# Check if GPUs are available\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        gpu_details = tf.config.experimental.get_device_details(gpu)\n",
    "        \n",
    "    # Use GPUtil to get detailed GPU information\n",
    "    gpus_info = GPUtil.getGPUs()\n",
    "    for gpu in gpus_info:\n",
    "        print(f\"GPU Model: {gpu.name}\")\n",
    "        print(f\"VRAM Total: {gpu.memoryTotal} MB\")\n",
    "        print(f\"VRAM Free: {gpu.memoryFree} MB\")\n",
    "        print(f\"VRAM Used: {gpu.memoryUsed} MB\")\n",
    "else:\n",
    "    print(\"No GPUs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db9f76-19f2-4c5b-96bf-4894de4a5681",
   "metadata": {},
   "source": [
    "# Set Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc4f9d-99c3-4e20-9321-37cf75404f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 120 #Number of days for matching procedure\n",
    "time_step = 20 \n",
    "holding_period = 10\n",
    "epochs = 64 \n",
    "\n",
    "stock = 'AAPL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a3f79-a72a-4b22-8d00-15b8826cd26f",
   "metadata": {},
   "source": [
    "# Euclidian Distance\n",
    "$d(\\vec{p},\\vec{q}) = \\sqrt{ \\sum_{i=1}^{H} (q_i - p_i)^2 }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1519a2-80d9-4565-985d-a5a74f22b880",
   "metadata": {},
   "source": [
    "# Euclidian Matching Algorithm on Price Action\n",
    "This function will take as input an element of $p = [0,1]^{H}$ and return $(x,y)$, a historical window of H days (from date $x$ to date $y$) which matches the price action of the test point the most.\n",
    "\n",
    "In essence, $$ r = \\underset{i}{\\arg \\min} \\quad d(p,Tr_{i})  $$ Where $Tr = \\{Tr_1, \\dots, Tr_N\\}$ and each $Tr_1, \\dots, Tr_N \\in [0,1]^{H}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8eb005-a20f-4231-a6f7-fb04217757ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(test_block, historical_blocks):\n",
    "\n",
    "    # Unpack the test block data\n",
    "    test_start_date, test_end_date, test_data_scaled = test_block\n",
    "\n",
    "    # Initialize variables to keep track of the minimum distance and the best matching block\n",
    "    min_distance = float('inf')\n",
    "    best_match = None\n",
    "    best_match_data = None\n",
    "\n",
    "    # Iterate over all historical blocks\n",
    "    for start_date, end_date, historical_data_scaled in historical_blocks:\n",
    "        # Compute the Euclidean distance between the test block and the current historical block\n",
    "        distance = euclidean(test_data_scaled.flatten(), historical_data_scaled.flatten())\n",
    "\n",
    "        # Update the minimum distance and best match if the current distance is smaller\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_match = (start_date, end_date)\n",
    "            best_match_data = historical_data_scaled\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66677e62-4f86-441f-8c4b-fc5c4617393a",
   "metadata": {},
   "source": [
    "# Retrieve Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747829d9-f0ba-4bba-871e-04a9aabf33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_block = yf.download(stock, start = '1990-01-01', end = '2014-12-31')[['Close']]\n",
    "train_sub_blocks = []\n",
    "\n",
    "for i in range(len(train_block) - H + 1):\n",
    "    datum = train_block['Close'][i:i+H].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(datum)\n",
    "    datum_scaled = scaler.transform(datum)\n",
    "\n",
    "    train_sub_blocks.append((train_block.index[i], train_block.index[i + H -1 ], datum_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4bd71-b384-4f4b-9b2d-293a6ff7b336",
   "metadata": {},
   "source": [
    "# Retrieve Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344e490-d3eb-4b6d-9108-15f9cfd38710",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_block = yf.download(stock, start = '2016-01-01', end = '2024-02-01')[['Close']] #1 year gap between train and test\n",
    "test_sub_blocks = []\n",
    "\n",
    "for i in range(len(test_block) - H +1):\n",
    "    datum = test_block['Close'][i:i+H].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(datum)\n",
    "    datum_scaled = scaler.transform(datum)\n",
    "\n",
    "    test_sub_blocks.append((test_block.index[i], test_block.index[i + H - 1], datum_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723165ef-8ba4-4bd8-899e-de5665c753b4",
   "metadata": {},
   "source": [
    "# ResNet Block for Skip Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a06da-75bf-452b-b721-5259ab1ea48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(x, filters, kernel_size, strides=(1, 1), activation='relu', padding='same'):\n",
    "    \"\"\"\n",
    "    A ResNet block with two convolutional layers and a skip connection.\n",
    "    \"\"\"\n",
    "    # Main path\n",
    "    y = Conv2D(filters, kernel_size, strides=strides, activation=activation, padding=padding)(x)\n",
    "    y = Conv2D(filters, kernel_size, strides=strides, activation=None, padding=padding)(y)\n",
    "    \n",
    "    # Skip connection\n",
    "    if x.shape[-1] != filters:\n",
    "        x = Conv2D(filters, kernel_size=(1, 1), strides=strides, activation=None, padding=padding)(x)\n",
    "    \n",
    "    # Add skip connection\n",
    "    out = Add()([x, y])\n",
    "    out = tf.keras.layers.Activation(activation)(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5aa33-1fb2-40b2-a9b6-5f257531dbcb",
   "metadata": {},
   "source": [
    "# Build Structure of the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ba7d3-3dd5-431e-b410-ec818766385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(time_step, 6, 1))\n",
    "    \n",
    "    # ResNet blocks\n",
    "    x = res_block(input_layer, 64, (5, 6))\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = res_block(x, 128, (3, 3))\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = res_block(x, 256, (1, 1))\n",
    "    x = MaxPooling2D(pool_size=(1, 1))(x)\n",
    "    \n",
    "    # Flatten the output of the pooling layer to feed into the dense layer\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(10, activation='relu')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)  # For binary classification\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b732f2a-4d07-4166-bd95-a86f685a6bf5",
   "metadata": {},
   "source": [
    "# Iteratively Train The Model\n",
    "1. Take in an input testing block, $\\vec{p} \\in [0,1]^H$.\n",
    "2. Find the historical dates $x,y$ corresponding to a signature vector $\\vec{q}^* \\in [0,1]^H$, where $\\vec{q}^* = \\underset{\\vec{q} \\in Tr}{\\arg \\min}  \\quad d(\\vec{p}, \\vec{q})$.\n",
    "3. Retrieve Open, High, Low, Close, Volume, 14d Momentum of the period $x-50$ to $y+50$, call this $M$.\n",
    "4. Roll a window over $M$ to split it into many $20\\times6$ matricies.\n",
    "5. Add these many $20 \\times 6$ matricies to our sample of training instances.\n",
    "6. Feed this new data into the existing model for **more training**.\n",
    "7. Form a $20 \\times 6$ matrix over the period of the tail end of the $\\vec{p}$, containing Open, High, Low, Close, Volume, 14d Momentum.\n",
    "8. Predict the Label of this testing datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a014c53-afff-47e8-9ff2-b28715494646",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "actuals = []\n",
    "rounded_preds= []\n",
    "returns = []\n",
    "temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0a275-23d4-4581-85bc-d4ed335c838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 50):\n",
    "    print('Prediction: ', i+1)\n",
    "    current_entry = test_sub_blocks[i]\n",
    "    most_similar_block_dates = match(current_entry, train_sub_blocks)\n",
    "    \n",
    "    # Gather data for most similar historical period\n",
    "    highest_similarity = yf.download(stock, \n",
    "                                     start = most_similar_block_dates[0].date() - timedelta(days = 50), \n",
    "                                     end = most_similar_block_dates[1].date() + timedelta(days = 50))[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    highest_similarity['Momentum'] = highest_similarity['Close'].diff(14) #14d momentum\n",
    "\n",
    "    # Find the labels for each day in the historical period\n",
    "    labels = []\n",
    "    buys = []\n",
    "    sells = []\n",
    "    for i in range(len(highest_similarity)-holding_period):\n",
    "        buy_price = highest_similarity['Open'][i+1]\n",
    "        sell_price = highest_similarity['Close'][i+holding_period]\n",
    "        buys.append(buy_price)\n",
    "        sells.append(sell_price)\n",
    "        if sell_price >= buy_price:\n",
    "            labels.append(1)\n",
    "        elif sell_price < buy_price:\n",
    "            labels.append(0)\n",
    "\n",
    "    # Match shape of lists\n",
    "    for i in range(len(highest_similarity) - len(labels)):\n",
    "        labels.append(None)\n",
    "        buys.append(None)\n",
    "        sells.append(None)\n",
    "\n",
    "    # Add lists to dataframe\n",
    "    highest_similarity['Label'] = labels\n",
    "    highest_similarity['Buy'] = buys\n",
    "    highest_similarity['Sell'] = sells\n",
    "\n",
    "    # Remove NAs and reset index\n",
    "    highest_similarity = highest_similarity.dropna()\n",
    "    highest_similarity.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # Format training and testing split \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(len(highest_similarity)-time_step):\n",
    "        datum = np.array(highest_similarity[['Open', 'High', 'Low', 'Close', 'Volume', 'Momentum']][i:i+time_step])\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(datum)\n",
    "        datum_scaled = scaler.transform(datum)\n",
    "        datum_scaled = datum_scaled.reshape(time_step, 6, 1)  # Reshape to match the input shape expected by the model\n",
    "        X_train.append(datum_scaled)\n",
    "        y_train.append(highest_similarity['Label'][i+time_step-1])\n",
    "\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # Fit the model (two choices)\n",
    "    # model = create_model() #[you can choose to fit a new model each day]\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=64, verbose=0)\n",
    "\n",
    "    # Get the financial data for the current period\n",
    "    current = yf.download(stock, \n",
    "                          start =  current_entry[1].date() - timedelta(days=70),  \n",
    "                          end =  current_entry[1].date() +timedelta(days =20) )[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    current['Momentum'] = current['Close'].diff(14)\n",
    "\n",
    "    # Find the labels for each day in the current period\n",
    "    labels = []\n",
    "    buys = []\n",
    "    sells = []\n",
    "    for i in range(len(current)-holding_period):\n",
    "        buy_price = current['Open'][i+1]\n",
    "        sell_price = current['Close'][i+holding_period]\n",
    "        \n",
    "        buys.append(buy_price)\n",
    "        sells.append(sell_price)\n",
    "        \n",
    "        if sell_price >= buy_price:\n",
    "            labels.append(1)\n",
    "        elif sell_price < buy_price:\n",
    "            labels.append(0)\n",
    "\n",
    "    # Match shape of lists\n",
    "    for i in range(len(current) - len(labels)):\n",
    "        labels.append(None)\n",
    "        buys.append(None)\n",
    "        sells.append(None)\n",
    "\n",
    "    # Add lists to dataframe\n",
    "    current['Label'] = labels\n",
    "    current['Buy'] = buys\n",
    "    current['Sell'] = sells\n",
    "\n",
    "    # Drop NA values\n",
    "    current = current.dropna()\n",
    "\n",
    "    # Ensure the index is a DatetimeIndex\n",
    "    current.index = pd.to_datetime(current.index)\n",
    "    \n",
    "    # Obtain the target date from current_entry and convert to pd.Timestamp\n",
    "    target_date = pd.Timestamp(current_entry[1].date())\n",
    "    \n",
    "    # Check if the date exists in the index to avoid KeyError\n",
    "    target_index = current.index.get_loc(target_date)\n",
    "    \n",
    "    # Calculate the start index for slicing\n",
    "    start_index = max(0, target_index - 20)\n",
    "    \n",
    "    # Slice the dataframe\n",
    "    subset_df = current.iloc[start_index + 1:target_index + 1]\n",
    "\n",
    "    # Get the matrix of the last 20 days from today\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    datum = np.array(subset_df[['Open', 'High', 'Low', 'Close', 'Volume', 'Momentum']])\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(datum)\n",
    "    datum_scaled = scaler.transform(datum)\n",
    "    datum_scaled = datum_scaled.reshape((time_step, 6, 1))  # Ensure correct reshaping\n",
    "    X_test.append(datum_scaled)\n",
    "    y_test.append(subset_df['Label'][-1])\n",
    "\n",
    "    # Store the returns \n",
    "    returns.append( (subset_df['Sell'][-1] - subset_df['Buy'][-1])/subset_df['Buy'][-1] )\n",
    "\n",
    "    # Make prediction\n",
    "    pred = model.predict(X_test[-1].reshape(1, 20, 6, 1))\n",
    "    print(pred)\n",
    "\n",
    "    # Store data for backtesting\n",
    "    temp.append( (current_entry[1].date(), pred, subset_df['Label'][-1]) )\n",
    "\n",
    "    # Store actual label\n",
    "    actuals.append(y_test[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83273f95-ca83-4121-a8e0-e20f84caa64a",
   "metadata": {},
   "source": [
    "# Make the Rounded Predictions over a certain Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6d270-bf77-4a13-9f3b-974699616190",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_buy = 0.9\n",
    "stock_date_label = []\n",
    "rounded_preds = []\n",
    "\n",
    "for i in range (len(temp)):\n",
    "    if temp[i][1][0][0] >= threshold_buy:\n",
    "        rounded_pred = 1\n",
    "    else:\n",
    "        rounded_pred =0\n",
    "    rounded_preds.append(rounded_pred)\n",
    "    stock_date_label.append( (temp[i][0], rounded_pred, temp[i][-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b40d4-1df4-4c5d-8d65-99bbb9044e39",
   "metadata": {},
   "source": [
    "# Backtest on Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24740f42-01cd-43cd-a036-f4811b7ae28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial balance and tracking variables\n",
    "initial_balance = 10000\n",
    "strategy_balance = [initial_balance]\n",
    "baseline_balance = [initial_balance]\n",
    "\n",
    "# Track strategy earnings\n",
    "bought = -10\n",
    "for i in range(len(rounded_preds)):\n",
    "    if rounded_preds[i] == 1 and i > bought + 9:\n",
    "        bought = i\n",
    "        strategy_balance.append(strategy_balance[-1] * (1 + returns[i]))\n",
    "    else:\n",
    "        strategy_balance.append(strategy_balance[-1])\n",
    "\n",
    "# Obtain baseline close prices and calculate returns\n",
    "stock_data = yf.download(stock, start=start_date, end=end_date)\n",
    "stock_data['Returns'] = stock_data['Close'].pct_change().fillna(0)\n",
    "baseline_balance = [initial_balance * (1 + stock_data['Returns'].cumsum().iloc[i]) for i in range(len(stock_data))]\n",
    "\n",
    "# Plotting strategy vs. baseline\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(strategy_balance, label='Strategy Earnings')\n",
    "plt.plot(baseline_balance, label='Baseline Earnings', linestyle='--')\n",
    "plt.title('Strategy vs. Baseline Earnings')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Balance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculating maximum drawdown\n",
    "def max_drawdown(equity_curve):\n",
    "    \"\"\"Calculate the maximum drawdown.\"\"\"\n",
    "    drawdowns = []\n",
    "    peak = equity_curve[0]\n",
    "    for value in equity_curve:\n",
    "        if value > peak:\n",
    "            peak = value\n",
    "        drawdown = (peak - value) / peak\n",
    "        drawdowns.append(drawdown)\n",
    "    return max(drawdowns)\n",
    "\n",
    "print('Maximum Drawdown: ', round(max_drawdown(strategy_balance) * 100, 2), '%')\n",
    "\n",
    "# Precision of the strategy\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "\n",
    "for i in range(len(rounded_preds)):\n",
    "    if rounded_preds[i] == 1 and actuals[i] == 1:\n",
    "        tp += 1\n",
    "    elif rounded_preds[i] == 1 and actuals[i] == 0:\n",
    "        fp += 1\n",
    "    elif rounded_preds[i] == 0 and actuals[i] == 0:\n",
    "        tn += 1\n",
    "    elif rounded_preds[i] == 0 and actuals[i] == 1:\n",
    "        fn += 1\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "print('Strategy Precision: ', round(precision * 100, 2), '%')\n",
    "\n",
    "# Store baseline performance\n",
    "baseline_returns = stock_data['Returns']\n",
    "baseline_tp = actuals.count(1)\n",
    "baseline_fp = len(actuals)-actuals.count(1)\n",
    "baseline_precision = baseline_tp / (baseline_tp + baseline_fp) if (baseline_tp + baseline_fp) != 0 else 0\n",
    "\n",
    "print('Baseline Precision: ', round(baseline_precision * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc49f1-95e6-466e-8f86-d96f1a88f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counters for correct, incorrect and total trades\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "trades = 0\n",
    "\n",
    "# Backtesting logic\n",
    "bought = -10\n",
    "initial = 10000\n",
    "balance = initial\n",
    "for i in range(len(rounded_preds)):\n",
    "    if rounded_preds[i] ==1 and i> bought + 9:\n",
    "        trades+= 1\n",
    "        bought = i\n",
    "        balance = balance + balance * returns[i]\n",
    "        \n",
    "        if returns[i]>0:\n",
    "            correct += 1\n",
    "            percents_win.append(returns[i])\n",
    "        elif returns[i]<= 0:\n",
    "            incorrect +=1\n",
    "            percents_loss.append(returns[i])\n",
    "\n",
    "print('Starting Balance: $', initial)\n",
    "print('End Balance: $', round(balance,2))\n",
    "print('ROI: ', round( ((balance-initial)/initial)*100,2),'%')\n",
    "print('Total Trades: ', trades)\n",
    "print('Resultant Precision: ', round( (correct/(correct+incorrect))*100,2), '%')\n",
    "print('######################')\n",
    "print('Overall Precision : ', round((tp/(tp+fp))*100,2), '%')\n",
    "print('Overall Accuracy : ', round(( (tp +tn)/(tp + fp + tn + fn))*100,2), '%')\n",
    "print('Average % Change When Win ', np.mean(percents_win))\n",
    "print('Average % Change When Lose ', np.mean(percents_loss))\n",
    "win_rate = tp/(tp+fp)\n",
    "print('Average Expected Value: ', win_rate*np.mean(percents_win) + (1-win_rate)*np.mean(percents_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
